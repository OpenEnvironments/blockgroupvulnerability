#! /usr/bin/env python
# -*- coding: utf-8 -*-
"""
These functions define the process for casting the CDC's SVI to the Census block
group level.  See the README.md for more context. 
    blockgroupvulnerability_getSVI      - reads in the CDC's annual SVI file, at the tract level
    blockgroupvulnerability_getACS      - pulls the familiar ACS variables from Census API at the requested geo level
    blockgroupvulnerability_getTIGER    - collects the TIGER geodatabases into a dataframe at the requested geo level
    blockgroupvulnerability_gdbtocsv    - a utility that removes the geometry from TIGER files for smaller CSVs
    blockgroupvulnerability_derive      - applies the CDC's formulas to calculate 2 dozen rules from ACS vars
    blockgroupvulnerability_train       - trains a model for each SVI theme
    blockgroupvulnerability_merge       - combines SVI, ACS and TIGER data and derives favorites (popdensity,eg)
    blockgroupvulnerability_predict     - applies the trained models to the tract level data
    blockgroupvulnerability_performance - provides accuracy report withthe RMSE of each theme
    blockgroupvulnerability_write       - writes the new SVI data to a csv file
    blockgroupvulnerability_params      - uses grid search to find the best parameters for the xgboost regression

"""

import pandas as pd
import numpy as np
import json
import requests
import math
from functools import reduce
from datetime import datetime
import os
import warnings

warnings.simplefilter('ignore')
    
    
def blockgroupvulnerability_getSVI(svidir, year, form = 'csv'):
    """
    This function retrieves the original CDC's SVI publication
    for the given year. Its shared as either a CSV file or as
    a geodatabase with one layer. 
    
    :param svidir: A string path to the CSV fil or geodb folder
    :return svi:   A dataframe with SVI variables and themes for the select year
    """    
    print("Getting the CDCs SVI publication at the tract level")
    # At this time, I cannot find an ftp site or other wget source
    # so the files need to be manually downloaded from:
    # https://www.atsdr.cdc.gov/placeandhealth/svi/data_documentation_download.html
        
    if form == 'csv':
        svi = pd.read_csv(svidir + year + "//SVI" + year + "_US.csv")
    elif form == 'gdb':
        import geopandas as gpd
        svi = gpd.read_file(svidir + year + "//SVI" + year + "_US")
    else:
        raise ValueError("getSVI form parameter should be 'csv' or 'gdb'")

    themes = ['RPL_THEME1', 'RPL_THEME2', 'RPL_THEME3', 'RPL_THEME4', 'RPL_THEMES']
    
    svikeys = ['ST','STATE','ST_ABBR','STCNTY','COUNTY','FIPS','LOCATION']
    for c in svi.columns:
        if c in svikeys:
            svi[c] = svi[c].astype(str)
        else:
            svi[c] = svi[c].astype(float)
    
    svi = svi[~svi.isin([-999]).any(axis=1)] # NA obs where population = 0
    svi = svi[svi['ST_ABBR'] != '72']  # Ignore Puerto Rico
    svi['geoid'] = svi.FIPS.str.zfill(11)
    
    return svi


def blockgroupvulnerability_getACS(acsvars, year, geog):
    """
    This function retrieves the ACS data needed for calculating the SVIs 23 own variables.
    The ACS variables come from Data Profiles, Subject Tables and Detail Tables. Only
    the last of these is available at the block group level, so the first two are joined 
    to it. As a result the tract level Data Profiles and tract level Subject Tables are
    have percentile ranking results shared across the block groups within each tract.    
    """
    print("Getting ACS variables for",year,'at the',geog,'level')
   
    states = pd.read_csv('https://raw.githubusercontent.com/OpenEnvironments/core/main/states.csv', dtype=str)
    states['StateFIPS'] = states.StateFIPS.str.zfill(2)

    if geog == 'tract':
        geolevels = ['state', 'county', 'tract']
    elif geog == 'block group':
        geolevels = ['state', 'county', 'tract', 'block group']
    else:
        raise ValueError("Unknown geography:"+geog)

    def divide_chunks(l, n):
        for i in range(0, len(l), n):
            yield l[i:i + n]

    acschunks = []
    c = 0
    for acschunk in divide_chunks(acsvars, 49):
        c +=1
        print("Chunk",c,"of",len(list(divide_chunks(acsvars, 50))))
        detailtables = []
        subjecttables = []
        dataprofiles = []
        dt = pd.DataFrame(columns=(['NAME']+geolevels))
        st = pd.DataFrame(columns=(['NAME']+geolevels))
        dp = pd.DataFrame(columns=(['NAME']+geolevels))
        for state in list(states[~states.CensusName.isna()].StateFIPS):
            print(state,",",end='')
            if geog == 'block group':
                geogkey  = "&for=block%20group:*&in=state:"+state+"&in=county:*&in=tract:*"
                geogkey += "&key=d75043ce01f4feafcc09f2a72ad3c80eb9567598"
            elif geog == 'tract':
                geogkey  = "&for=tract:*&in=state:"+state+"&in=county:*"
                geogkey += "&key=d75043ce01f4feafcc09f2a72ad3c80eb9567598"
            else:
                raise ValueError("Unknown geography:"+geog)
            # Detail Tables
            if len([c for c in acschunk if "B" in c])>0:
                dtreq = "https://api.census.gov/data/"+year+"/acs/acs5?get=NAME,"
                dtreq += ",".join([c for c in acschunk if "B" in c])
                dtreq += geogkey
                dtresult = requests.get(dtreq)
                detailtables += json.loads(dtresult.text)[1:]
            # Subject Tables
            if len([c for c in acschunk if "C" in c])>0:
                streq = "https://api.census.gov/data/"+year+"/acs/acs5/subject?get=NAME,"
                streq += ",".join([c for c in acschunk if "S" in c])
                streq += geogkey
                stresult = requests.get(streq)
                subjecttables += json.loads(stresult.text)[1:]
            # Data Profiles
            if len([c for c in acschunk if "D" in c])>0:
                dpreq = "https://api.census.gov/data/"+year+"/acs/acs5/profile?get=NAME,"
                dpreq += ",".join([c for c in acschunk if "D" in c])
                dpreq += geogkey
                dpresult = requests.get(dpreq)
                dataprofiles +=  json.loads(dpresult.text)[1:]
        print()
        if len(detailtables)>0:
            dt = pd.DataFrame(columns=json.loads(dtresult.text)[0],data=detailtables)
        if len(subjecttables)>0:
            st = pd.DataFrame(columns=json.loads(stresult.text)[0],data=subjecttables)
        if len(dataprofiles)>0:
            dp = pd.DataFrame(columns=json.loads(dpresult.text)[0],data=dataprofiles)
        tmp1 = pd.merge(dt,st.drop('NAME',axis=1),on=geolevels,how='outer')
        tmp2 = pd.merge(tmp1,dp.drop('NAME',axis=1),on=geolevels,how='outer')
        acschunks.append(tmp2)
    
    acs = pd.concat([a for a in acschunks],axis=1)
    acs = acs.loc[:,~acs.columns.duplicated()].copy()
    
    acs['geoid'] = np.add.reduce(acs[geolevels].astype(str), axis=1)
        
    return acs


def blockgroupvulnerability_getTIGER(tldir, pdtype):
    """
    This function opens all the TIGER\Line shapefiles in the given
    directory and appends them to return a single dataframe.
    """
    print("Getting the TIGER data")

    tl = []
    if pdtype == 'gdb':
        import geopandas as gpd
        for filename in os.listdir(tldir+' '+pdtype):
            tl.append(gpd.read_file(os.path.join(tldir+' '+pdtype, filename)))
    elif pdtype == 'csv':
        import pandas as pd
        for filename in os.listdir(tldir+' '+pdtype):
            tl.append(pd.read_csv(os.path.join(tldir+' '+pdtype, filename)))        
    else:
        raise ValueError("pdtype must be 'csv' or 'gdb'")

    tiger = pd.concat(tl)
    tiger['geoid'] = tiger.GEOID.astype(str).str.zfill(11)
    
    return tiger


def blockgroupvulnerability_gdbtocsv(tldir):
    """
    For posterity, this function opens the tiger gdb files and saves them, without geometry
    as CSV files
    """
    print('Converting gbd to csv files')
    
    import geopandas as gpd
    import pandas as pd
    import os
    
    for filename in os.listdir(tldir+' gdb'):
        print('Reading', filename)
        tlfile = gpd.read_file(os.path.join(tldir+' gdb', filename))
        tlfile.drop('geometry',axis=1).to_csv(os.path.join(tldir+' csv', filename+'.csv'), compression=None)   
                                              
    return None
                                              
                                              
def blockgroupvulnerability_derive(svitractdf):
    """
    This function applies the CDC's formulas for each of 23 CDC Variables
    supporting the SVI Themes.    
    """
    svivars = ["sngpnt","limeng","disabl","age65","age17","noveh","munit","mobile","groupq","crowd","uninsur","unemp","pov150","nohsdp","hburd","twomore","nhpi","minrty","hisp","asian","aian","afam"]
    svitractdf['sngpnt']  = svitractdf['B11012_010E'] + svitractdf['B11012_015E']
    svitractdf['limeng']  = svitractdf['B16005_007E'] + svitractdf['B16005_008E'] + svitractdf['B16005_012E'] + svitractdf['B16005_013E'] + svitractdf['B16005_017E'] + svitractdf['B16005_018E'] + svitractdf['B16005_022E'] + svitractdf['B16005_023E'] + svitractdf['B16005_029E'] + svitractdf['B16005_030E'] + svitractdf['B16005_034E'] + svitractdf['B16005_035E'] + svitractdf['B16005_039E'] + svitractdf['B16005_040E'] + svitractdf['B16005_044E'] + svitractdf['B16005_045E']
    svitractdf['disabl']  = svitractdf['DP02_0072E']
    svitractdf['age65']   = svitractdf['S0101_C01_030E']
    svitractdf['age17']   = svitractdf['B09001_001E']
    svitractdf['noveh']   = svitractdf['DP04_0058E']
    svitractdf['munit']   = svitractdf['DP04_0012E'] + svitractdf['DP04_0013E']
    svitractdf['mobile']  = svitractdf['DP04_0014E']
    svitractdf['groupq']  = svitractdf['B26001_001E']
    svitractdf['crowd']   = svitractdf['DP04_0078E'] + svitractdf['DP04_0079E']
    svitractdf['uninsur'] = svitractdf['S2701_C04_001E']
    svitractdf['unemp']   = svitractdf['DP03_0005E']
    svitractdf['pov150']  = svitractdf['S1701_C01_040E']
    svitractdf['nohsdp']  = svitractdf['B06009_002E']
    svitractdf['hburd']   = svitractdf['S2503_C01_028E'] + svitractdf['S2503_C01_032E'] + svitractdf['S2503_C01_036E'] + svitractdf['S2503_C01_040E']
    svitractdf['twomore'] = svitractdf['DP05_0083E']
    svitractdf['otherrace'] = svitractdf['DP05_0082E'] # E_OTHERRACE is no longer included
    svitractdf['nhpi']    = svitractdf['DP05_0081E']
    svitractdf['minrty']  = svitractdf['DP05_0071E'] + svitractdf['DP05_0078E'] + svitractdf['DP05_0079E'] + svitractdf['DP05_0080E'] + svitractdf['DP05_0081E'] + svitractdf['DP05_0082E'] + svitractdf['DP05_0083E'] 
    svitractdf['hisp']    = svitractdf['DP05_0071E']
    svitractdf['asian']   = svitractdf['DP05_0080E']
    svitractdf['aian']    = svitractdf['DP05_0079E']
    svitractdf['afam']    = svitractdf['DP05_0078E']
    svitractdf['noint']   = svitractdf['S2802_C01_001E'] - svitractdf['S2802_C02_001E']

    # Calculate the percentile ranks for each of the SVI Variables
    def percentile_rank(svidf,col,ascending=True):
        """
        This function accepts a dataframe with 2 columns:
            FIPS code is unique key
            col is the column whose value is ranked

        It caclulates the target column's percentile rank given as: 
            Percentile Rank = (Rank-1) / (N-1)

        The function returns a response dataframe that has just the original key, as well as a new
        column prefixed by "PCTRANK_"
        """
        temp = svidf[['FIPS','E_'+col]].sort_values('E_'+col, ascending=ascending)
        temp.reset_index(drop=True, inplace=True)
        temp['RANK_'+col] = temp.index + 1
        temp.drop('E_'+col,axis=1,inplace=True)
        temp['PCTRANK_'+col] = (temp['RANK_'+col] - 1) / (temp.shape[0]-1)
        return temp

    for c in svivars:
        print("Ranking",c)
        rank = percentile_rank(svitract,c)
        svitractsd = pd.merge(svitractdf,rank,on='FIPS',how='inner')

    svitractsd["E_THEME1"] = svitractsd[["PCTRANK_UNINSUR","PCTRANK_UNEMP","PCTRANK_POV150","PCTRANK_NOHSDP","PCTRANK_HBURD"]].sum(axis=1)
    svitractsd["E_THEME2"] = svitractsd[["PCTRANK_SNGPNT","PCTRANK_LIMENG","PCTRANK_DISABL","PCTRANK_AGE65","PCTRANK_AGE17"]].sum(axis=1)
    svitractsd["E_THEME3"] = svitractsd[['PCTRANK_TWOMORE','PCTRANK_OTHERRACE','PCTRANK_NHPI','PCTRANK_MINRTY','PCTRANK_HISP','PCTRANK_ASIAN','PCTRANK_AIAN','PCTRANK_AFAM']].sum(axis=1)
    svitractsd["E_THEME4"] = svitractsd[['PCTRANK_NOVEH','PCTRANK_MUNIT','PCTRANK_MOBILE','PCTRANK_GROUPQ','PCTRANK_CROWD']].sum(axis=1)

    for c in ['1','2','3','4']:
        print("Ranking",'THEME'+c)
        rank = percentile_rank(svitract,'THEME'+c)
        svitractsd = pd.merge(svitractdf,rank,on='FIPS',how='inner')

    return svitractsd

          
def blockgroupvulnerability_paramsearch():
    """
    This function uses GridSearchCV to fund the best parameters for the XGBoost regressor
    used in the blockgroupvulnerability_train models. The shared best params are based on the
    results for modelling the composite theme 'RPL_THEMES'.
    
    It is not used in the main flow. But provides the templar code that yields the parameters
    implemented in blcokgroupvulnerability_train.
    
    """
    warnings.simplefilter(action='ignore', category=FutureWarning)

    trainingset = trainingset.copy()
    trainingset.dropna(axis=0, how='any', inplace=True) 

    from sklearn.model_selection import train_test_split
    from sklearn import preprocessing
    from sklearn.model_selection import GridSearchCV
    import xgboost as xgb
    from xgboost.sklearn import XGBRegressor
    from pandas import MultiIndex, Int16Dtype, Int64Index

    y = trainingset['RPL_THEMES']
    X = trainingset[modelvars]

    #
    # Ready to model at the tract level
    #

    print('searching best params')

    model = XGBRegressor(seed=1234)   # tree_method='gpu_hist' to use gpu
    parameters = {'nthread':[4], # when use hyperthread, xgboost may become slower
                  'objective':['reg:logistic'],       # ['reg:linear'],
                  'learning_rate': [0.1, 0.01, 0.05], #so called `eta` value
                  'max_depth': range (2, 10, 1),
                  'min_child_weight': [4],
                  'silent': [1],
                  'subsample': [0.7],
                  'colsample_bytree': [0.7],
                  'n_estimators':  range(60, 220, 40)}

    xgb_grid = GridSearchCV(estimator = model,
                            param_grid = parameters,
                            scoring = 'roc_auc',
                            cv = 10,     # cross validation folds
                            n_jobs = 12, # based on (2/3s) num of CPUs, 16 on the yogi host
                            verbose=True)

    xgb_grid.fit(X, y)

    print(xgb_grid.best_score_)
    print(xgb_grid.best_params_)

    return

def blockgroupvulnerability_train(train):
    """
    This function takes the prepared dataset with SVI, ACS, TIGER and derived columns,
    and builds a model for each SVI theme (target). It returns a dictionary of model
    result objects that can be used for prediction.
    """
    print('Training 5 models:')

    #
    # A General Linear Model
    #

    import numpy as np
    import math
    import statsmodels as sm
    
    train['const'] = 1  # intercept col needed for linear models
    
    targets = train[targetcols]
    X = train[modelvars]
    X.dropna(axis=0, how='any', inplace=True )

    results = {}
    for target in ['1','2','3','4','S']:

        print("    model:",'THEME'+target)
        y = targets['RPL_THEME'+target]

        #
        # General Linear Model with binomial family
        #
        #import statsmodels.api as sm
        #model = sm.GLM(y, X, family=sm.families.Binomial())
        #results['THEME'+target] = model.fit()

        #
        # XGBoost
        #
        import xgboost as xgb
        xgbparams = {'colsample_bytree': 0.7, 'learning_rate': 0.05, 'max_depth': 7, 'min_child_weight': 4, 'nthread': 4, 'objective': 'reg:logistic', 'subsample': 0.7} #'n_estimators': 500, 'silent': 1, 
        results['THEME'+target] = xgb.train(xgbparams, xgb.DMatrix(X, label=y), num_boost_round=10)

    return results

def blockgroupvulnerability_merge(acs,tiger,svi=pd.DataFrame()):
    """
    The ACS, TIGER\Line and SVI data need to be combined into a single dataframe for training.
    All but the SVI are needed for a similar merge to support prediction.
    These function,
     - merges these source dataframes on their shared geoid, tract level for training and blockgroup level for prediction
     - derived variables are calculated, as ALAND is added to ACS sources.
    Finally, the percentile rank and Z normalized versions of these variables are recaculated.
    """
    print('Merging')
    if len(svi.columns) == 0:
        master = pd.merge(tiger,acs,on='geoid',how='inner')
    else:
        master = pd.merge(tiger,pd.merge(svi,acs,on='geoid',how='inner'), on='geoid',how='inner')
     
    from oe_census import oe_derive_columns
    with open('myrulesformatted.txt','r') as r:
        rules = r.readlines()

    for col in (tractACSVARS + bgACSVARS + derivedVARS + targetcols):
        if col in master.columns:
            master[col] = master[col].astype(float)

    master, status = oe_derive_columns.oe_derive_columns(master, rules)

    for col in (tractACSVARS + bgACSVARS + derivedVARS):
        if col in master.columns:
            master["pr_"+col] = master[col].rank(pct=True)
            master["z_"+col] = (master[col] - master[col].mean()) / master[col].std() 
       
    return master

def blockgroupvulnerability_predict(models,exog):
    """
    This function accepts a set of predictive models and applies them to a new
    testing or actual (exogenous) dataset to return the prediction results for each
    SVI theme.
    """
    print('Predicting')
    
    print(exog.shape)
    exog['const'] = 1
    X = exog[modelvars+["geoid"]].dropna(axis=0, how='any')
    
    prediction = pd.DataFrame()
    prediction['GEOID'] = X['geoid']
    X.drop("geoid",axis=1,inplace=True)

    for target in targetcols:
        # prediction[target[4:]] = models[target[4:]].predict(exog[modelvars]).rank(pct=True)
        import xgboost as xgb
        prediction[target[4:]] = models[target[4:]].predict(xgb.DMatrix(X))
        
    for target in targetcols:
        prediction[target[4:]] = prediction[target[4:]].rank(pct=True)
        
    return prediction


def blockgroupvulnerability_performance(predicted, actual):
    # For each predicted theme, take its mean at the tract level and compare it to CDC's original
    from matplotlib import pyplot as plt
    import seaborn as sns

    predicted['tract'] = predicted['GEOID'].str[:-1]

    compare = pd.merge(predicted.groupby(['tract']).mean().reset_index(),
                       actual, left_on='tract', right_on='geoid', how='inner')

    report = []
    for theme in ['1','2','3','4','S']:
        MSE = np.square(np.subtract(compare['THEME'+theme],
                                    compare['RPL_THEME'+theme])).mean() 
        RMSE = math.sqrt(MSE)
        report.append({"Theme":'THEME'+theme,"RMSE":RMSE})
        %matplotlib inline
        plt.plot(compare['RPL_THEME'+theme], compare['THEME'+theme], 'o', alpha=0.2);
        plt.title("Theme "+theme)
        plt.show()

    print('Performance Report')
    print()
    print(pd.DataFrame(report).to_string(index=False))          
    return


def blockgroupvulnerability_write(df,year):
    """
    This function writes the blockgroupvulnerability projection
    to a csv file for publication.    
    """
    print('Writing to file')
    df.to_csv(year + 'blockgroupvulnerability.csv', index=False)
    return None


#if __name__ == '__main__':
def main():
    
    YEAR = '2020'
    SVIDIR = "D:\\Open Environments\\data\\cdc\\svi\\"
    TIGERDIR = 'D:\\Open Environments\\data\\census\\tiger\\2020 tracts'
    # ACS variables available at the block group level
    bgACSVARS = ["B01003_001E","B01001_001E","B01001_002E","B01001_003E","B01001_004E","B01001_005E","B01001_006E","B01001_007E","B01001_008E","B01001_009E","B01001_010E","B01001_011E","B01001_012E","B01001_013E","B01001_014E","B01001_015E","B01001_016E","B01001_017E","B01001_018E","B01001_019E","B01001_020E","B01001_021E","B01001_022E","B01001_023E","B01001_024E","B01001_025E","B01001_026E","B01001_027E","B01001_028E","B01001_029E","B01001_030E","B01001_031E","B01001_032E","B01001_033E","B01001_034E","B01001_035E","B01001_036E","B01001_037E","B01001_038E","B01001_039E","B01001_040E","B01001_041E","B01001_042E","B01001_043E","B01001_044E","B01001_045E","B01001_046E","B01001_047E","B01001_048E","B01001_049E","B19013_001E","B19001_001E","B19001_002E","B19001_003E","B19001_004E","B19001_005E","B19001_006E","B19001_007E","B19001_008E","B19001_009E","B19001_010E","B19001_011E","B19001_012E","B19001_013E","B19001_014E","B19001_015E","B19001_016E","B19001_017E","B15003_001E","B15003_002E","B15003_003E","B15003_004E","B15003_005E","B15003_006E","B15003_007E","B15003_008E","B15003_009E","B15003_010E","B15003_011E","B15003_012E","B15003_013E","B15003_014E","B15003_015E","B15003_016E","B15003_017E","B15003_018E","B15003_019E","B15003_020E","B15003_021E","B15003_022E","B15003_023E","B15003_024E","B15003_025E","B02001_001E","B02001_002E","B02001_003E","B02001_004E","B02001_005E","B02001_006E","B02001_007E","B02001_008E","B03003_001E","B03003_002E","B03003_003E","B11001_001E","B11001_002E","B11001_007E","B23025_001E","B23025_002E","B23025_007E","B09019_001E","B09018_001E"]
    # ACS vars available at the tract level (used by the CDC to calc its own basis vars)
    tractACSVARS = ['B11012_010E', 'B11012_015E', 'B16005_007E', 'B16005_008E', 'B16005_012E', 'B16005_013E', 'B16005_017E', 'B16005_018E', 'B16005_022E', 'B16005_023E', 'B16005_029E', 'B16005_030E', 'B16005_034E', 'B16005_035E', 'B16005_039E', 'B16005_040E', 'B16005_044E', 'B16005_045E', 'DP02_0072E', 'S0101_C01_030E', 'B09001_001E', 'DP04_0058E', 'DP04_0012E', 'DP04_0013E', 'DP04_0014E', 'B26001_001E', 'DP04_0078E', 'DP04_0079E', 'S2701_C04_001E', 'DP03_0005E', 'S1701_C01_040E', 'B06009_002E', 'S2503_C01_028E', 'S2503_C01_032E', 'S2503_C01_036E', 'S2503_C01_040E', 'DP05_0083E', 'DP05_0082E', 'DP05_0081E', 'DP05_0071E', 'DP05_0078E', 'DP05_0079E', 'DP05_0080E']   
    derivedVARS = ['Over65Rate','MinorityRate','PopDensity','CollegePlusRate','EmploymentRate','FamilyRate','ChildrenRate','Under25KRate','logPop','logPopDensity','logLand','logRaceBasis']
    keycols = ['state','county','tract', 'STATEFP', 'COUNTYFP', 'TRACTCE', 'GEOID', 'geoid', 'NAME_y', 'NAME_x', 'NAMELSAD', 'MTFCC', 'FUNCSTAT', 'INTPTLAT', 'INTPTLON', 'ST', 'STATE', 'ST_ABBR', 'STCNTY', 'COUNTY', 'FIPS', 'LOCATION']
    targetcols = ['RPL_THEME1','RPL_THEME2','RPL_THEME3','RPL_THEME4','RPL_THEMES']
    modelvars = ['const']
    for c in (bgACSVARS + derivedVARS):
        modelvars.append(c)
        modelvars.append("pr_"+c)
        modelvars.append("z_"+c)
    modelvars = [c for c in modelvars if c not in ['z_logRaceBasis','z_logPopDensity','z_logLand','z_logPop']]
    
    # Get the training data then train the model  (tract level)
    svitract = blockgroupvulnerability_getSVI(SVIDIR, YEAR, form='csv')
    svitract.to_pickle('svitract.pkl')
    acstract = blockgroupvulnerability_getACS(bgACSVARS, YEAR, 'tract')
    acstract.to_pickle('acstract.pkl')
    tigertract = blockgroupvulnerability_getTIGER('D:\\Open Environments\\data\\census\\tiger\\2020 tracts', pdtype='csv')
    tigertract.to_pickle('tigertract.pkl')
    trainingset = blockgroupvulnerability_merge(acstract,tigertract,svitract)
    
    models = blockgroupvulnerability_train(trainingset)

    # Get the data for prediction then predict (blockgroup level)
    acsblockgroup = blockgroupvulnerability_getACS(bgACSVARS, YEAR, 'block group')
    acsblockgroup.to_pickle('acsblockgroup.pkl')
    tigerblockgroup = blockgroupvulnerability_getTIGER('D:\\Open Environments\\data\\census\\tiger\\2020 blockgroups', pdtype='csv')
    tigerblockgroup.to_pickle('tigerblockgroup.pkl')
    predictionset = blockgroupvulnerability_merge(acsblockgroup,tigerblockgroup)
    sviblockgroup = blockgroupvulnerability_predict(models,predictionset)

    blockgroupvulnerability_write(sviblockgroup,YEAR)
    blockgroupvulnerability_performance(sviblockgroup, svitract)
    
    print('Done')
